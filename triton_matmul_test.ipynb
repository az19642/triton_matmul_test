{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install triton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaVP-8FBO6ZY",
        "outputId": "512e2504-bfa6-43e8-915e-8721f5dacde3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl"
      ],
      "metadata": {
        "id": "oxHtnyOsOxW_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_autotune_config():\n",
        "    return [\n",
        "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}),\n",
        "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}),\n",
        "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}),\n",
        "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}),\n",
        "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}),\n",
        "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}),\n",
        "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}),\n",
        "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8})\n",
        "    ]"
      ],
      "metadata": {
        "id": "S3YXg-B0q0Cs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.autotune(\n",
        "    configs=get_autotune_config(),\n",
        "    key=['M', 'N', 'K'],\n",
        ")\n",
        "@triton.jit\n",
        "def matmul_kernel(\n",
        "        a_ptr, b_ptr, c_ptr,\n",
        "        M, N, K,\n",
        "        stride_am, stride_ak,\n",
        "        stride_bk, stride_bn,\n",
        "        stride_cm, stride_cn,\n",
        "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
        "        GROUP_SIZE_M: tl.constexpr\n",
        "):\n",
        "    \"\"\"Kernel for computing the matmul C = A x B.\n",
        "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
        "    \"\"\"\n",
        "    pid = tl.program_id(axis=0)\n",
        "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
        "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
        "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
        "    group_id = pid // num_pid_in_group\n",
        "    first_pid_m = group_id * GROUP_SIZE_M\n",
        "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
        "    pid_m = first_pid_m + (pid % group_size_m)\n",
        "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
        "\n",
        "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
        "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
        "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
        "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
        "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
        "\n",
        "\n",
        "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
        "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
        "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
        "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
        "        accumulator = tl.dot(a, b, accumulator)\n",
        "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
        "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
        "    c = accumulator.to(tl.float16)\n",
        "\n",
        "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
        "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
        "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
        "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
        "    tl.store(c_ptrs, c, mask=c_mask)"
      ],
      "metadata": {
        "id": "DXUqFPViqvue"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matmul(a, b):\n",
        "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
        "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
        "    M, K = a.shape\n",
        "    K, N = b.shape\n",
        "\n",
        "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
        "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
        "    matmul_kernel[grid](\n",
        "        a, b, c,\n",
        "        M, N, K,\n",
        "        a.stride(0), a.stride(1),\n",
        "        b.stride(0), b.stride(1),\n",
        "        c.stride(0), c.stride(1),\n",
        "    )\n",
        "    return c"
      ],
      "metadata": {
        "id": "nT3dH6IirXST"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref_lib = 'cuBLAS'\n",
        "\n",
        "configs = []\n",
        "\n",
        "configs.append(\n",
        "    triton.testing.Benchmark(\n",
        "        x_names=[\"K\"],\n",
        "        x_vals=[i for i in range(512, 1000)],\n",
        "        line_arg=\"provider\",\n",
        "        line_vals=[ref_lib.lower(), \"triton\"],\n",
        "        line_names= [ref_lib, \"Triton\"],\n",
        "        styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
        "        ylabel=\"Time (ms)\",\n",
        "        plot_name=\"matmul-performance-\" +\n",
        "        (\"fp16\"),\n",
        "        args={\"M\": 8192, \"N\": 8192}\n",
        "    ))\n",
        "\n",
        "\n",
        "@triton.testing.perf_report(configs)\n",
        "def benchmark(M, N, K, provider):\n",
        "    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n",
        "    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n",
        "    quantiles = [0.5, 0.2, 0.8]\n",
        "    if provider == ref_lib.lower():\n",
        "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n",
        "    if provider == 'triton':\n",
        "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n",
        "\n",
        "    return ms, max_ms, min_ms\n",
        "\n",
        "benchmark.run(show_plots=True, print_data=True)"
      ],
      "metadata": {
        "id": "NlhUaK_FsRxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cBugObnDteg5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}